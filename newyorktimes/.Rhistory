section <- search_nyt$results$section
author <- search_nyt$results$byline
author[sapply(author, function(x){length(x)==0})] <- NA
title <- search_nyt$results$title
abstract <- search_nyt$results$abstract
abstract[sapply(abstract, function(x){length(x)==0})] <- NA
date <- search_nyt$results$published_date
new_result <- data.frame('url' = unlist(url),'category' = unlist(section), 'author' = unlist(author),'title' = unlist(title), 'abstract' = unlist(abstract), 'date' = unlist(date))
return(new_result)
}
mostpoplar()
mostpoplar <- function(type='emailed', period=1){
library(httr)
library(stringr)
library(tidyverse)
library(jsonlite)
if(((type != '(shared|viewed|emailed)')) | !is.null(type)){
stop("input must be specific character: shared, viewed, emailed")
}
if((period != 1) | (period != 7) | (period != 30) |(! is.null(period))){
stop("input must be specific number: 1, 7, 30")
}
if(!is.numeric(period)){
stop("input must be a numeric number")
}
search_url <- 'https://api.nytimes.com/svc/mostpopular/v2/'
base_url <- paste(search_url, type, '/', period, '.', 'json', sep = '')
search_r <- GET(base_url, query = list('api-key'=Sys.getenv("NYT_KEY")))
search_c <- content(search_r)
search_nyt <- fromJSON(toJSON(search_c))
url <- search_nyt$results$url
section <- search_nyt$results$section
author <- search_nyt$results$byline
author[sapply(author, function(x){length(x)==0})] <- NA
title <- search_nyt$results$title
abstract <- search_nyt$results$abstract
abstract[sapply(abstract, function(x){length(x)==0})] <- NA
date <- search_nyt$results$published_date
new_result <- data.frame('url' = unlist(url),'category' = unlist(section), 'author' = unlist(author),'title' = unlist(title), 'abstract' = unlist(abstract), 'date' = unlist(date))
return(new_result)
}
mostpoplar()
mostpoplar <- function(type='emailed', period=1){
library(httr)
library(stringr)
library(tidyverse)
library(jsonlite)
if((type != '(shared|viewed|emailed)') | (!(type ==''))){
stop("input must be specific character: shared, viewed, emailed")
}
if((period != 1) | (period != 7) | (period != 30) |(!(period == ''))){
stop("input must be specific number: 1, 7, 30")
}
if(!is.numeric(period)){
stop("input must be a numeric number")
}
search_url <- 'https://api.nytimes.com/svc/mostpopular/v2/'
base_url <- paste(search_url, type, '/', period, '.', 'json', sep = '')
search_r <- GET(base_url, query = list('api-key'=Sys.getenv("NYT_KEY")))
search_c <- content(search_r)
search_nyt <- fromJSON(toJSON(search_c))
url <- search_nyt$results$url
section <- search_nyt$results$section
author <- search_nyt$results$byline
author[sapply(author, function(x){length(x)==0})] <- NA
title <- search_nyt$results$title
abstract <- search_nyt$results$abstract
abstract[sapply(abstract, function(x){length(x)==0})] <- NA
date <- search_nyt$results$published_date
new_result <- data.frame('url' = unlist(url),'category' = unlist(section), 'author' = unlist(author),'title' = unlist(title), 'abstract' = unlist(abstract), 'date' = unlist(date))
return(new_result)
}
mostpoplar()
mostpoplar <- function(type='emailed', period=1){
library(httr)
library(stringr)
library(tidyverse)
library(jsonlite)
if((type != '(shared|viewed|emailed)') | (!(type ==''))){
stop("input must be specific character: shared, viewed, emailed")
}
if((period != 1) | (period != 7) | (period != 30) |(!(period == ''))){
stop("input must be specific number: 1, 7, 30")
}
if(!is.numeric(period)){
stop("input must be a numeric number")
}
search_url <- 'https://api.nytimes.com/svc/mostpopular/v2/'
base_url <- paste(search_url, type, '/', period, '.', 'json', sep = '')
search_r <- GET(base_url, query = list('api-key'=Sys.getenv("NYT_KEY")))
search_c <- content(search_r)
search_nyt <- fromJSON(toJSON(search_c))
url <- search_nyt$results$url
section <- search_nyt$results$section
author <- search_nyt$results$byline
author[sapply(author, function(x){length(x)==0})] <- NA
title <- search_nyt$results$title
abstract <- search_nyt$results$abstract
abstract[sapply(abstract, function(x){length(x)==0})] <- NA
date <- search_nyt$results$published_date
new_result <- data.frame('url' = unlist(url),'category' = unlist(section), 'author' = unlist(author),'title' = unlist(title), 'abstract' = unlist(abstract), 'date' = unlist(date))
return(new_result)
}
mostpoplar()
mostpoplar('emailed',)
mostpoplar()
search_article()
search_article(1,1)
search_article('new',2)
mostpoplar <- function(type='emailed', period=1){
library(httr)
library(stringr)
library(tidyverse)
library(jsonlite)
if(!is.character(type)){
stop("input must be a character")
}
if(!is.numeric(period)){
stop("input must be a numeric")
}
search_url <- 'https://api.nytimes.com/svc/mostpopular/v2/'
base_url <- paste(search_url, type, '/', period, '.', 'json', sep = '')
search_r <- GET(base_url, query = list('api-key'=Sys.getenv("NYT_KEY")))
search_c <- content(search_r)
search_nyt <- fromJSON(toJSON(search_c))
url <- search_nyt$results$url
section <- search_nyt$results$section
author <- search_nyt$results$byline
author[sapply(author, function(x){length(x)==0})] <- NA
title <- search_nyt$results$title
abstract <- search_nyt$results$abstract
abstract[sapply(abstract, function(x){length(x)==0})] <- NA
date <- search_nyt$results$published_date
new_result <- data.frame('url' = unlist(url),'category' = unlist(section), 'author' = unlist(author),'title' = unlist(title), 'abstract' = unlist(abstract), 'date' = unlist(date))
return(new_result)
}
mostpoplar()
install.packages("devtools")
library(devtools)
load_all()
document()
document()
check()
load_all()
document()
check()
load_all()
document()
check()
load_all()
load_all()
document()
check()
use_package('httr')
use_package('httr')
use_package('httr', 'stringr', 'jsonlite', 'tidyr', 'tidyverse', 'ggplot2')
use_package('stringr')
use_package("jsonlite")
load_all()
document()
check()
??comment
?comment()
?search_article()
?mostpopular()
?comments()
load_all()
install.packages("devtools")
load_all()
library(devtools)
load_all()
document()
check()
load_all()
document()
check()
load_all()
document()
check()
load_all()
document()
check()
use_readme_rmd()
use_mit_license("LICENSE")
use_vignette("my-vignette")
library(newyorktimes)
search_article('new york', 20191231)
mostpopular("emailed", 1)
mostpopular("emailed", 1)
comments('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
best_recommend('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
load_all()
document()
check()
search_article('new york', 20191231)
check()
best_recommend('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
load_all()
document()
document()
comments('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
best_recommend('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
check()
rss_rank('Arts')
specific_articles(2019,1)
specific_articles(2019,1) %>%
head()
specific_articles(2019,1) %>%
head(n=10)
load_all()
document()
check()
devtools::build_vignettes()
devtools::build_vignettes()
library(newyorktimes)
search_article('new york', 20191231)
??search_article()
?search_artcile()
document()
??seach_article()
??comments
document()
??specific_articles()
library(newyorktimes)
install.packages('newyorktimes')
library(newyorktimes)
load_all()
install.packages("devtools")
library(devtools)
load_all()
document()
?best_recommend
?comments
?mostpopular
check()
library(devtools)
load_all()
use_package('httr')
use_package('stringr')
use_package('httr')
use_package('stringr')
use_package('tidyverse')
use_package('plyr')
use_package('tidyr')
use_package('jsonlite')
use_package('dplyr')
use_package('xml2')
load_all()
library(newyorktimes)
install.packages("newyorktimes")
load_all()
library(devtools)
load_all()
document()
library(newyorktimes)
# use search_article() function to find articles' information
search_article('new york', 20191231)
library(devtools)
load_all()
document()
check()
document()
check()
devtools::build_vignettes()
library(newyorktimes)
rss_rank('Arts')
rss_rank <- function(search_section='Arts'){
library(xml2)
library(stringr)
library(httr)
# Get XML
base <- 'https://api.nytimes.com/services/xml/rss/nyt/'
specific_url <- paste(base,search_section,'.xml', sep='')
resp_xml <- GET(specific_url)
# Examine returned text with content()
xml_text <- content(resp_xml, as = "text")
# Turn rev_text into an XML document
rss_xml <- read_xml(xml_text)
title <- xml_find_all(rss_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(rss_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(rss_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
author <- xml_find_all(rss_xml, xpath = '/rss/channel/item/dc:creator') %>%
xml_text()
public_date <- xml_find_all(rss_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rank <- c(1: length(title))
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'author' = author, 'public_date' = date, 'rank' = rank)
return(rss_data)
}
rss_rank()
rss_rank()
library(devtools)
check()
library(newyorktimes)
search_article('new york', 20191231)
mostpopular("emailed", 1)
comments('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
best_recommend('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
rss_rank('Arts')
rss_rank <- function(search_section){
library(xml2)
# Get XML
base <- 'https://api.nytimes.com/services/xml/rss/nyt/'
specific_url <- paste(base,search_section,'.xml', sep='')
resp_xml <- GET(specific_url)
# Examine returned text with content()
xml_text <- content(resp_xml, as = "text")
# Turn rev_text into an XML document
rss_xml <- read_xml(xml_text)
title <- xml_find_all(rss_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(rss_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(rss_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
author <- xml_find_all(rss_xml, xpath = '/rss/channel/item/dc:creator') %>%
xml_text()
public_date <- xml_find_all(rss_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rank <- c(1: length(title))
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'author' = author, 'public_date' = date, 'rank' = rank)
rss_data[,]
return(rss_data)
}
rss_rank('Arts')
rss_rank <- function(search_section){
library(xml2)
# Get XML
base <- 'https://api.nytimes.com/services/xml/rss/nyt/'
specific_url <- paste(base,search_section,'.xml', sep='')
resp_xml <- GET(specific_url)
# Examine returned text with content()
xml_text <- content(resp_xml, as = "text")
# Turn rev_text into an XML document
rss_xml <- read_xml(xml_text)
title <- xml_find_all(rss_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(rss_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(rss_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
author <- xml_find_all(rss_xml, xpath = '/rss/channel/item/dc:creator') %>%
xml_text()
public_date <- xml_find_all(rss_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rank <- c(1: length(title))
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'author' = author, 'public_date' = date, 'rank' = rank)
rss_data[,]
return(rss_data)
}
rss_rank('Arts')
rss_rank('Business')
rss_rank <- function(search_section){
library(xml2)
# Get XML
base <- 'https://api.nytimes.com/services/xml/rss/nyt/'
specific_url <- paste(base,search_section,'.xml', sep='')
resp_xml <- GET(specific_url)
# Examine returned text with content()
xml_text <- content(resp_xml, as = "text")
# Turn rev_text into an XML document
rss_xml <- read_xml(xml_text)
title <- xml_find_all(rss_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(rss_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(rss_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
author <- xml_find_all(rss_xml, xpath = '/rss/channel/item/dc:creator') %>%
xml_text()
public_date <- xml_find_all(rss_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rank <- c(1: length(title))
rss_data <- data.frame('title' = unlist(title), 'link' = unlist(link), 'description' = unlist(description), 'author' = unlist(author), 'public_date' = date, 'rank' = rank)
rss_data[,]
return(rss_data)
}
rss_rank('Business')
title <- xml_find_all(books_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
u <- 'https://api.nytimes.com/services/xml/rss/nyt/Arts.xml'
library(xml2)
# Get XML
resp_xml <- GET('https://api.nytimes.com/services/xml/rss/nyt/Arts.xml')
# Check response is XML
http_type(resp_xml)
# Examine returned text with content()
books_text <- content(resp_xml, as = "text")
# Turn rev_text into an XML document
books_xml <- read_xml(books_text)
books_xml
# Examine the structure of rev_xml
#xml_structure(books_xml)
title <- xml_find_all(books_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(books_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(books_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
author <- xml_find_all(books_xml, xpath = '/rss/channel/item/dc:creator') %>%
xml_text()
public_date <- xml_find_all(books_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'author' = author, 'public_date' = date)
title <- xml_find_all(books_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(books_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(books_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
public_date <- xml_find_all(books_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'public_date' = date)
rss_rank <- function(search_section){
library(xml2)
# Get XML
base <- 'https://api.nytimes.com/services/xml/rss/nyt/'
specific_url <- paste(base,search_section,'.xml', sep='')
resp_xml <- GET(specific_url)
# Examine returned text with content()
xml_text <- content(resp_xml, as = "text")
# Turn rev_text into an XML document
rss_xml <- read_xml(xml_text)
title <- xml_find_all(rss_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(rss_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(rss_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
public_date <- xml_find_all(rss_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rank <- c(1: length(title))
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'public_date' = date)
return(rss_data)
}
rss_rank('Business')
title <- xml_find_all(books_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(books_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(books_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
author <- xml_find_all(books_xml, xpath = '/rss/channel/item/dc:creator') %>%
xml_text()
author[sapply(author, function(x){length(x)==0})] <- NA
public_date <- xml_find_all(books_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'author' = author, 'public_date' = date)
title <- xml_find_all(books_xml, xpath = '/rss/channel/item/title') %>%
xml_text()
link <- xml_find_all(books_xml, xpath = '/rss/channel/item/link') %>%
xml_text()
description <- xml_find_all(books_xml, xpath = '/rss/channel/item/description') %>%
xml_text()
author <- xml_find_all(books_xml, xpath = '/rss/channel/item/dc:creator') %>%
xml_text()
author[sapply(author, function(x){length(x)==0})] <- NA
public_date <- xml_find_all(books_xml, xpath = '/rss/channel/item/pubDate') %>%
xml_text()
date <- str_extract(public_date, "[A-Z][a-z][a-z], [0-9][0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9]") #clean the date in the dataset
rss_data <- data.frame('title' = title, 'link' = link, 'description' = description, 'author' = unlist(author), 'public_date' = date)
load_all()
document()
rss_rank()
library(newyorktimes)
rss_rank()
load_all()
document()
document()
rss_rank()
library(devtools)
library(newyorktimes)
search_article()
rss_rank("Arts")
load_all()
document()
check()
build_vignettes()
library(newyorktimes)
rss_rank('Arts')
build_vignettes()
best_recommend('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
check()
load_all()
document()
check()
load_all()
document()
check()
best_recommend('https://www.nytimes.com/2019/12/08/opinion/donald-trump-impeachment.html')
rss_rank('Arts')
specific_articles(2019,1) %>%
head(n=10)
mostpopular("emailed", 1)
search_article('new york', 20191231)
build_vignettes()
load_all()
document()
build_vignettes()
check()
check()
